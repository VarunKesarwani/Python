{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsHV+HPYP2IX6W6meGKwWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VarunKesarwani/Python/blob/master/ResumeATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BqGKHpNERga",
        "outputId": "7536bb36-5829-42a9-f5e0-832b92c60052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=2f64b59cbcf959e5b711f9b3f84bbcd811f28bb2b9e3f3977021b6f7ae5fd0fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ],
      "source": [
        "pip install docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "LPd-wTunEgYz",
        "outputId": "f157f1bc-c13a-432e-ce00-f6726f6dcc62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-196176dd-6965-4149-a8d3-24b3b48985ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-196176dd-6965-4149-a8d3-24b3b48985ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BhavanaGuptaResume.docx to BhavanaGuptaResume.docx\n",
            "Saving Overview.docx to Overview.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt\n",
        "resume = docx2txt.process(\"BhavanaGuptaResume.docx\")\n",
        "print(resume)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVUg4PEAFrBK",
        "outputId": "f036a6d5-01a2-42b9-edfb-b65bf425d7fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "About Me\n",
            "\n",
            "An experienced data engineer with 13 years of expertise in the conception, development, deployment, and upkeep of robust, efficient, and highly accessible data pipelines and datasets ensuring data quality. Proficient in distributed systems frameworks such as PySpark, Hadoop, Kafka, Hive, AWS, Presto, and Tableau, as well as ETL tools like Ab Initio, Talend, and Unix. Skilled in PL/SQL, actively engaged in Channel SaaS strategy, and proficient in Python programming. Demonstrated leadership in technical teams, ensuring project success through best practices, Teamwork, effective communication, and comprehensive documentation.\n",
            "\n",
            "\n",
            "\n",
            "Certifications\n",
            "\n",
            "AWS Certified Cloud Practitioner (CLF-C02)\n",
            "\n",
            "           Oct 2023\n",
            "\n",
            "\n",
            "\n",
            "Tableau Desktop Specialist (TDS-C01)\n",
            "\n",
            "           Sept 2023\n",
            "\n",
            "\n",
            "\n",
            "Technical Skills\n",
            "\n",
            "PySpark \n",
            "\n",
            "Python\n",
            "\n",
            "Hadoop\n",
            "\n",
            "Apache Spark\n",
            "\n",
            "HDFS\n",
            "\n",
            "Talend\n",
            "\n",
            "Kafka\n",
            "\n",
            "AWS\n",
            "\n",
            "Unix\n",
            "\n",
            "SQL\n",
            "\n",
            "HQL\n",
            "\n",
            "Hive\n",
            "\n",
            "Teradata\n",
            "\n",
            "Oracle\n",
            "\n",
            "Tableau\n",
            "\n",
            "Ab-Initio\n",
            "\n",
            "Git\n",
            "\n",
            "\n",
            "\n",
            "Other Prowess\n",
            "\n",
            "SaaS/ Banking Domain\n",
            "\n",
            "Data Pipeline Designing \n",
            "\n",
            "AML Analytics/ Fraud Detection\n",
            "\n",
            "strategic & Critical thinking \n",
            "\n",
            "Product Owner/ Delivery\n",
            "\n",
            "\n",
            "\n",
            "Personal Info\n",
            "\n",
            "\n",
            "\n",
            "https://www.linkedin.com/in/bhavana-gupta-r/\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "804, T-6, North Town,\n",
            "\n",
            "4,5,6,7 Stephenson Rd\n",
            "\n",
            "Perambur,\n",
            "\n",
            "Chennai - 600012\n",
            "\n",
            "\n",
            "\n",
            "BHAVANA GUPTA\n",
            "\n",
            "Senior Data Engineer\n",
            "\n",
            " rbhavanagupta@outlook.com\n",
            "\n",
            "9884975569\n",
            "\n",
            "Chennai, Tamil Nadu, India\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Work Experience\n",
            "\n",
            "\n",
            "\n",
            "VMWare \n",
            "\n",
            "Mar 2022 - Present\n",
            "\n",
            "\n",
            "\n",
            "Senior Data Engineer\n",
            "\n",
            "Performed a thorough analysis of SaaS data specific to Partner channel, extracting valuable insights, and actively engaged in discussions with both the business and operations teams to formulate a channel strategy for SaaS. Created MRR metrics and ARR bridge as a POC for Channel SaaS program. Travelled to US twice for SaaS (Software as a service) workshops.\n",
            "\n",
            "\n",
            "\n",
            "Participated in Sprint rituals and engaged in architecture design creating a data solution for Partner Connect program, aiming to revamp the partner experience, motivate, and reward partners for their focus on multi-cloud opportunities, enabling around 25K+ VMware partners. \n",
            "\n",
            "\n",
            "\n",
            "Tata Consultancy Services\n",
            "\n",
            "Nov 2010 – Feb 2022\n",
            "\n",
            "Senior Data Engineer\n",
            "\n",
            "Citi Bank NA, APAC, EMEA| Singapore/India\n",
            "\n",
            "Participated in the complete delivery of data engineering projects, involving the design and construction of data pipelines for batch processing in Hadoop using PySpark, as well as conducting AML and fraud detection analysis using HQL. This encompassed various tasks, such as Hadoop data ingestion from Teradata databases using Sqoop and migrating SAS code to Hive through HQL. Furthermore, I achieved a successful execution of a Proof of Concept (POC) for real-time log streaming using Apache Kafka within the Spark framework. Was responsible for security and integrity of data following SOPs.\n",
            "\n",
            "\n",
            "\n",
            "ETL Lead \n",
            "\n",
            "Citi Bank North America| Mexico/India\n",
            "\n",
            "Have effectively led a team of 6 members, directing the execution of multiple ETL projects using Ab Initio, Unix, and Teradata, and demonstrated my ability to collaborate seamlessly with cross-functional teams, including business analysts, data modelers, QA, and reporting. My experience includes extensive client interaction and project management, ensuring the successful implementation of projects.\n",
            "\n",
            "\n",
            "\n",
            "ETL Developer \n",
            "\n",
            "Citi Bank North America| India\n",
            "\n",
            "Engaged in the process of gathering requirements with business stakeholders, meticulously analyzed these requirements, and documented them in the form of a Low-Level Design (LLD) document. Took on the responsibility for developing data transformations that closely adhered to the business logic. Played a pivotal role throughout the entire testing phase and the subsequent production implementation.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Education\n",
            "\n",
            "BE\n",
            "\n",
            "09/2006 - 05/2010\n",
            "\n",
            "Electrical and Electronic Engineering\n",
            "\n",
            "Anna University \n",
            "\n",
            "University 21st rank (gold medalist)\n",
            "\n",
            "\n",
            "\n",
            "High School \n",
            "\n",
            "03/2005 - 04/2006\n",
            "\n",
            "PCM - Biology \n",
            "\n",
            "State Board, Tamil Nadu \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "proact\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_desc = docx2txt.process(\"Overview.docx\")\n",
        "print(job_desc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SywRVsOnGGYl",
        "outputId": "1694e3cf-6ec9-4c6a-c18c-2fedf588d532"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overview\n",
            "\n",
            "Microsoft Office is at the center of Microsoft’s efforts to empower our users to do more. Across mobile, web, and desktop, empowering users to create, edit, and interact with their documents, embracing a core belief that you should have the best of breed experience irrespective of device. The Data, Insights, and Growth (DIG) team – part of the Office Fuel organization powering engineering capability and customer lifecycle – builds transformative capabilities that help Microsoft teams immerse in understanding, protecting, and empowering the customer via data. We deliver innovative modern data solutions and services which enable new Office experiences at service velocity, intelligent analysis, and richly informed hypothesis-driven agility – driving business success through customer engagement and satisfaction. \n",
            "\n",
            "We offer a fantastic and fun team environment with a commitment to people, inclusion. We celebrate team cultural values of being: \n",
            "\n",
            "Customer-obsessed (and helping everyone else live this) \n",
            "\n",
            "Exceptional partners – designing and co-developing capabilities for and across dozens (to hundreds) of teams \n",
            "\n",
            "Committed to continual learning and growing in building a high-trust, collaborative, diverse, and deeply inclusive team environment. \n",
            "\n",
            "We are looking for an experienced Data Engineer to join our Insights team in DIG. We empower Office Engineering Teams with Data and Insights to let them respond to customer feedback, steer future investments, and ensure our customers have the best experience possible daily.  We leverage state-of-the-art Azure technologies for big data processing tech that is available internally and open source/Azure technologies such as Synapse, Spark, Azure Data Lake, Azure Data Explorer, Azure Data Factory etc. Great opportunity to impact millions of Office Customers, shape the data analytics platform and systems architecture, and contribute to the success of Office product group.  \n",
            "\n",
            "Qualifications\n",
            "\n",
            "Bachelor/Master degree in Mathematics, Statistics, Data science, Computer Science, Computer Engineering, or related field \n",
            "\n",
            "6+ years of hands-on experience with big data technologies as well as data analytics tools, SQL or any querying language, Data Engineering technologies, ETL \n",
            "\n",
            "Experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python \n",
            "\n",
            "Excellent data visualization skills to be able to present insights that drive business impact \n",
            "\n",
            "Excellent communication & collaboration skills  \n",
            "\n",
            "Responsibilities\n",
            "\n",
            "Contribute to the strategy, vision, architecture, and execution for our data platform and self-serve data analytics systems \n",
            "\n",
            "Coach and mentor engineers, both technically through code and design reviews \n",
            "\n",
            "Lead the collaborate with data scientists, developers, and business stakeholders to understand data requirements and develop innovative solutions \n",
            "\n",
            "Build data processing pipelines that are both efficient and reliable by leveraging a variety of technologies such as Azure Data Factory, Azure Synapse, Azure Data Explorer, Azure Data Lake Store Gen 2, Cosmos & Scope Scripts, ClickHouse, Superset and other technologies \n",
            "\n",
            "Collaboration with Office Engineering teams to fulfill the data requirements to meet the needs of internal and external partners, ultimately contributing to the consumer user growth \n",
            "\n",
            "Collaboration with the Data Science team in building and productizing the data science models for Growth User Lifecycle, including acquisition and engagement needs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [resume,job_desc]"
      ],
      "metadata": {
        "id": "7UQw5hBmGaHZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "count_matrix = cv.fit_transform(text)"
      ],
      "metadata": {
        "id": "wIZKSIhwGn1C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(cosine_similarity(count_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwwzr3rgG-bV",
        "outputId": "39e8c1d1-ab12-43ef-e285-aed8bc647c53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.70630238]\n",
            " [0.70630238 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_pert = cosine_similarity(count_matrix)[0][1] * 100\n",
        "match_pert = round(match_pert,2)\n",
        "\n",
        "print(match_pert)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS4gDfKqHNcl",
        "outputId": "73604c28-59ed-4270-fba1-fae02511046a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names_out(input_features=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEBC_9g5Kfb6",
        "outputId": "e22f3c06-b6ec-40ae-9643-0417eb76f1db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['03', '04', '05', '09', '13', '2005', '2006', '2010', '2022',\n",
              "       '2023', '21st', '25k', '600012', '804', '9884975569', 'ab',\n",
              "       'ability', 'able', 'about', 'accessible', 'achieved',\n",
              "       'acquisition', 'across', 'actively', 'adhered', 'agility',\n",
              "       'aiming', 'america', 'aml', 'an', 'analysis', 'analysts',\n",
              "       'analytics', 'analyzed', 'and', 'anna', 'any', 'apac', 'apache',\n",
              "       'architecture', 'are', 'around', 'arr', 'art', 'as', 'at',\n",
              "       'available', 'aws', 'azure', 'bachelor', 'bank', 'banking',\n",
              "       'batch', 'be', 'being', 'belief', 'best', 'bhavana', 'big',\n",
              "       'biology', 'board', 'both', 'breed', 'bridge', 'build', 'building',\n",
              "       'builds', 'business', 'but', 'by', 'c01', 'c02', 'capabilities',\n",
              "       'capability', 'celebrate', 'center', 'certifications', 'certified',\n",
              "       'channel', 'chennai', 'citi', 'clf', 'clickhouse', 'client',\n",
              "       'closely', 'cloud', 'co', 'coach', 'code', 'coding', 'collaborate',\n",
              "       'collaboration', 'collaborative', 'com', 'commitment', 'committed',\n",
              "       'communication', 'complete', 'comprehensive', 'computer',\n",
              "       'concept', 'conception', 'conducting', 'connect', 'construction',\n",
              "       'consultancy', 'consumer', 'continual', 'contribute',\n",
              "       'contributing', 'core', 'cosmos', 'create', 'created', 'creating',\n",
              "       'critical', 'cross', 'cultural', 'customer', 'customers', 'daily',\n",
              "       'data', 'databases', 'datasets', 'deeply', 'degree', 'deliver',\n",
              "       'delivery', 'demonstrated', 'deployment', 'design', 'designing',\n",
              "       'desktop', 'detection', 'develop', 'developer', 'developers',\n",
              "       'developing', 'development', 'device', 'dig', 'directing',\n",
              "       'discussions', 'distributed', 'diverse', 'do', 'document',\n",
              "       'documentation', 'documented', 'documents', 'domain', 'dozens',\n",
              "       'drive', 'driven', 'driving', 'edit', 'education', 'effective',\n",
              "       'effectively', 'efficient', 'efforts', 'electrical', 'electronic',\n",
              "       'else', 'embracing', 'emea', 'empower', 'empowering', 'enable',\n",
              "       'enabling', 'encompassed', 'engaged', 'engagement', 'engineer',\n",
              "       'engineering', 'engineers', 'ensure', 'ensuring', 'entire',\n",
              "       'environment', 'etc', 'etl', 'everyone', 'excellent',\n",
              "       'exceptional', 'execution', 'experience', 'experienced',\n",
              "       'experiences', 'expertise', 'explorer', 'extensive', 'external',\n",
              "       'extracting', 'factory', 'fantastic', 'feb', 'feedback', 'field',\n",
              "       'focus', 'following', 'for', 'form', 'formulate', 'framework',\n",
              "       'frameworks', 'fraud', 'from', 'fuel', 'fulfill', 'fun',\n",
              "       'functional', 'furthermore', 'future', 'gathering', 'gen', 'git',\n",
              "       'gold', 'great', 'group', 'growing', 'growth', 'gupta', 'hadoop',\n",
              "       'hands', 'have', 'hdfs', 'help', 'helping', 'high', 'highly',\n",
              "       'hive', 'hql', 'https', 'hundreds', 'hypothesis', 'immerse',\n",
              "       'impact', 'implementation', 'in', 'includes', 'including',\n",
              "       'inclusion', 'inclusive', 'india', 'info', 'informed', 'ingestion',\n",
              "       'initio', 'innovative', 'insights', 'integrity', 'intelligent',\n",
              "       'interact', 'interaction', 'internal', 'internally', 'investments',\n",
              "       'involving', 'irrespective', 'is', 'java', 'javascript', 'join',\n",
              "       'kafka', 'lake', 'language', 'languages', 'lead', 'leadership',\n",
              "       'learning', 'led', 'let', 'level', 'leverage', 'leveraging',\n",
              "       'lifecycle', 'like', 'limited', 'linkedin', 'live', 'lld', 'log',\n",
              "       'logic', 'looking', 'low', 'management', 'mar', 'master',\n",
              "       'mathematics', 'me', 'medalist', 'meet', 'members', 'mentor',\n",
              "       'meticulously', 'metrics', 'mexico', 'microsoft', 'migrating',\n",
              "       'millions', 'mobile', 'modelers', 'models', 'modern', 'more',\n",
              "       'motivate', 'mrr', 'multi', 'multiple', 'my', 'na', 'nadu',\n",
              "       'needs', 'new', 'north', 'not', 'nov', 'obsessed', 'oct', 'of',\n",
              "       'offer', 'office', 'on', 'open', 'operations', 'opportunities',\n",
              "       'opportunity', 'or', 'oracle', 'organization', 'other', 'our',\n",
              "       'outlook', 'overview', 'owner', 'part', 'participated', 'partner',\n",
              "       'partners', 'pcm', 'people', 'perambur', 'performed', 'personal',\n",
              "       'phase', 'pipeline', 'pipelines', 'pivotal', 'pl', 'platform',\n",
              "       'played', 'poc', 'possible', 'powering', 'practices',\n",
              "       'practitioner', 'present', 'presto', 'proact', 'process',\n",
              "       'processing', 'product', 'production', 'productizing',\n",
              "       'proficient', 'program', 'programming', 'project', 'projects',\n",
              "       'proof', 'protecting', 'prowess', 'pyspark', 'python', 'qa',\n",
              "       'qualifications', 'quality', 'querying', 'rank', 'rbhavanagupta',\n",
              "       'rd', 'real', 'related', 'reliable', 'reporting', 'requirements',\n",
              "       'respond', 'responsibilities', 'responsibility', 'responsible',\n",
              "       'revamp', 'reviews', 'reward', 'richly', 'rituals', 'robust',\n",
              "       'role', 'saas', 'sas', 'satisfaction', 'school', 'science',\n",
              "       'scientists', 'scope', 'scripts', 'seamlessly', 'security', 'self',\n",
              "       'senior', 'sept', 'serve', 'service', 'services', 'shape',\n",
              "       'should', 'singapore', 'skilled', 'skills', 'software', 'solution',\n",
              "       'solutions', 'sops', 'source', 'spark', 'specialist', 'specific',\n",
              "       'sprint', 'sql', 'sqoop', 'stakeholders', 'state', 'statistics',\n",
              "       'steer', 'stephenson', 'store', 'strategic', 'strategy',\n",
              "       'streaming', 'subsequent', 'success', 'successful', 'such',\n",
              "       'superset', 'synapse', 'systems', 'tableau', 'talend', 'tamil',\n",
              "       'tasks', 'tata', 'tds', 'team', 'teams', 'teamwork', 'tech',\n",
              "       'technical', 'technically', 'technologies', 'teradata', 'testing',\n",
              "       'that', 'the', 'their', 'them', 'these', 'thinking', 'this',\n",
              "       'thorough', 'through', 'throughout', 'time', 'to', 'took', 'tools',\n",
              "       'town', 'transformations', 'transformative', 'travelled', 'trust',\n",
              "       'twice', 'ultimately', 'understand', 'understanding', 'university',\n",
              "       'unix', 'upkeep', 'us', 'user', 'users', 'using', 'valuable',\n",
              "       'values', 'variety', 'various', 'velocity', 'via', 'vision',\n",
              "       'visualization', 'vmware', 'was', 'we', 'web', 'well', 'which',\n",
              "       'with', 'within', 'work', 'workshops', 'www', 'years', 'you'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_params(deep=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvAbomLQLsX3",
        "outputId": "fedc6aae-5698-438d-809a-865a9ba7752e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'analyzer': 'word',\n",
              " 'binary': False,\n",
              " 'decode_error': 'strict',\n",
              " 'dtype': numpy.int64,\n",
              " 'encoding': 'utf-8',\n",
              " 'input': 'content',\n",
              " 'lowercase': True,\n",
              " 'max_df': 1.0,\n",
              " 'max_features': None,\n",
              " 'min_df': 1,\n",
              " 'ngram_range': (1, 1),\n",
              " 'preprocessor': None,\n",
              " 'stop_words': None,\n",
              " 'strip_accents': None,\n",
              " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'tokenizer': None,\n",
              " 'vocabulary': None}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}